{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw98ZqWwBfCe"
   },
   "source": [
    "## **Assignment 3 - Modeling**\n",
    "## CS585 - NLP\n",
    "## Oleksandr Shashkov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZlL26oZ385L"
   },
   "source": [
    "### The goal of this final phase of the project is to build a text categorization model on your primary dataset, and to evaluate it on both your primary and your secondary dataset.  Follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gCsUsPd4VP7"
   },
   "source": [
    "**1. Data partitioning:** Create a Training set for your model by randomly selecting 70% of the texts in your PRIMARY dataset.  Use the remaining 30% of texts from the PRIMARY dataset as your Test (PRIMARY) set.  Designate 100% of your SECONDARY dataset as the Test (SECONDARY) dataset.  So you should have one Training set (drawn from the PRIMARY data), and two different Test sets (one from PRIMARY and one from SECONDARY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncIBJzft4gsS",
    "outputId": "43417d51-b31e-4eb3-f40b-2e2629c81ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8hCJBF74g6p",
    "outputId": "efa999c6-2cc3-451f-a326-d2dd32bd3165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary dataset shape: (813, 2)\n",
      "                                                text  label\n",
      "0  I would be much more likely to patronize a bus...   True\n",
      "1  Show me the real research that supports this. ...  False\n",
      "2  One of the commenters said it best.   Business...  False\n",
      "3  Start requiring antibody titers on all employe...   True\n",
      "4  The Nashville picture says it all. Unmasked pe...  False\n",
      "\n",
      "\n",
      "Secondary dataset shape: (900, 2)\n",
      "                                                text  label\n",
      "0  Putin After Announcing #CovidVaccine #Russian ...   True\n",
      "1  Courtesy: WA! #WhatsApp #COVID #CovidVaccine h...   True\n",
      "2  4 of the vaccines Jared bought are expected to...   True\n",
      "3  One day you will realize CDC Guidelines magica...  False\n",
      "4  Im far from lying.  Current CDC guidelines is ...   True\n"
     ]
    }
   ],
   "source": [
    "primary_set_path = \"/content/drive/MyDrive/Education/CS585/HW3/PRIMARY/nyt_topic_vaccination.csv\"\n",
    "secondary_set_path = \"/content/drive/MyDrive/Education/CS585/HW3/SECONDARY/twitter_topic_vaccination.csv\"\n",
    "# load primary dataset\n",
    "primary_dataset = pd.read_csv(primary_set_path)\n",
    "#this is to shuffle dataset\n",
    "primary_dataset = primary_dataset.sample(frac=1,random_state=55).reset_index(drop=True)\n",
    "# load secondary dataset\n",
    "secondary_dataset = pd.read_csv(secondary_set_path)\n",
    "# quickly explore datasets:\n",
    "print('Primary dataset shape: ' + str(primary_dataset.shape))\n",
    "print(primary_dataset.head(5))\n",
    "print('\\n')\n",
    "print('Secondary dataset shape: ' + str(secondary_dataset.shape))\n",
    "print(secondary_dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lCIuQQdjToER"
   },
   "outputs": [],
   "source": [
    "test_size_percentage = 0.7\n",
    "num_of_train_samples = int(primary_dataset.shape[0]*test_size_percentage)\n",
    "# primary dataset:\n",
    "x_primary = primary_dataset.text\n",
    "y_primary = primary_dataset.label.astype(int)\n",
    "x_prim_train = x_primary[:num_of_train_samples,]\n",
    "y_prim_train = y_primary[:num_of_train_samples,]\n",
    "x_prim_test = x_primary[num_of_train_samples:,]\n",
    "y_prim_test = y_primary[num_of_train_samples:,]\n",
    "# secondary dataset:\n",
    "x_sec_test = secondary_dataset.text\n",
    "y_sec_test = secondary_dataset.label.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHFoe8Ew4hrp"
   },
   "source": [
    "**2. Baseline model training:** Train a simple bag-of-words classifier on your Training dataset.  If your data comes from the stance task, you will build a multiclass model (one which can assign one of three labels - pro-mitigation, anti-mitigation, or unclear). **If your data comes from the topic task, choose only one of the topics (masking and distancing, lockdowns, vaccination) to model as a binary classification task.  (You should avoid topics with low numbers of positive examples.)**  An example of how to use scikit-learn to build a simple text categorization model is [here](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51Xr7Ys65JiT",
    "outputId": "ed338897-c74d-44fa-f45f-283de4cb3305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vectorize and normalize the data\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",lowercase=True).fit(x_prim_train)\n",
    "print(\"Vocabulary size: {}\\n\".format(len(vectorizer.vocabulary_)))\n",
    "x_prim_train_transformed = vectorizer.transform(x_prim_train)\n",
    "x_prim_test_transformed = vectorizer.transform(x_prim_test)\n",
    "x_sec_test_transformed = vectorizer.transform(x_sec_test)\n",
    "#build and train the model on training data from primary dataset\n",
    "lr_model = LogisticRegression().fit(x_prim_train_transformed, y_prim_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVK1UwY95KVB"
   },
   "source": [
    "**3. Model evaluation 1:** Calculate your baseline model's accuracy for your model's predictions on the Test (PRIMARY) set, and on the Test (SECONDARY) set.  Enter these values in the answer boxes provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSpUpmAF5dcC",
    "outputId": "d58b7910-473c-4906-b7e2-240a572ffaf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy of the model on primary dataset: 80.74%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.56      0.70        98\n",
      "           1       0.77      0.97      0.86       146\n",
      "\n",
      "    accuracy                           0.81       244\n",
      "   macro avg       0.85      0.77      0.78       244\n",
      "weighted avg       0.83      0.81      0.79       244\n",
      "\n",
      "Baseline accuracy of the model on secondary dataset: 72.44%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68       375\n",
      "           1       0.78      0.74      0.76       525\n",
      "\n",
      "    accuracy                           0.72       900\n",
      "   macro avg       0.72      0.72      0.72       900\n",
      "weighted avg       0.73      0.72      0.73       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline performance on primary dataset\n",
    "baseline_accuracy_prim = lr_model.score(x_prim_test_transformed,y_prim_test)\n",
    "print(\"Baseline accuracy of the model on primary dataset: {:.2%}\\n\".format(baseline_accuracy_prim))\n",
    "print(classification_report(y_prim_test, lr_model.predict(x_prim_test_transformed)))\n",
    "# baseline performance on secondary dataset\n",
    "baseline_accuracy_sec = lr_model.score(x_sec_test_transformed,y_sec_test)\n",
    "print(\"Baseline accuracy of the model on secondary dataset: {:.2%}\\n\".format(baseline_accuracy_sec))\n",
    "print(classification_report(y_sec_test, lr_model.predict(x_sec_test_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJqInWvnrbc-"
   },
   "source": [
    "Note: \n",
    ">  low recall for class 0 on primary test data and lower precision for class 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpsChF7Y5eJx"
   },
   "source": [
    "**4. Feature engineering:** In order to try to improve your model, think about what features of the text might be associated with the category you are trying to predict.  What attributes of a text besides the presence of individual words might be good predictors (for example, regular expression patterns or specific word sequences)?  Create at least three new features that represent attributes of the text.  Add them to your model and retrain.  An example of how to add a set of features (defined as a vector of 1/0 values indicating whether the attribute is present or absent for a given text) is shown [here](https://gist.github.com/DerrickHiggins/20c77745b080e3d493231424d7da9a2f)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyqmt5QdDiX1"
   },
   "source": [
    "Let's retrieve misclassified data to see how the model can be helped to process it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWVeTUM96A1L",
    "outputId": "507f3691-ec08-4af4-a7f6-c2b5c658d82b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  prediction\n",
      "0  Closing things because of Covid-19 has hurt a ...      0           1\n",
      "1  Why is this sliding into 2nd amendment kind of...      0           1\n",
      "2  I’m fully vaxed and gladly don my very comfort...      0           1\n",
      "3  Expose the antivaxxer crowd as the disruptive ...      0           1\n",
      "4  We have not eaten at a restaurant since early ...      1           0\n",
      "                                                text  label  prediction\n",
      "0  Putin After Announcing #CovidVaccine #Russian ...      1           0\n",
      "1  4 of the vaccines Jared bought are expected to...      1           0\n",
      "2  I don’t see masks. In fact, my child was there...      0           1\n",
      "3  will the sense touch 40k after a #CovidVaccine...      1           0\n",
      "4  These patients are slowly being tortured and t...      0           1\n"
     ]
    }
   ],
   "source": [
    "y_test = np.asarray(y_prim_test)\n",
    "y_hat_prim_test = lr_model.predict(x_prim_test_transformed)\n",
    "misclassified_indx = np.where(y_test != y_hat_prim_test)\n",
    "misclassified_data = pd.DataFrame()\n",
    "misclassified_indx_corrected = misclassified_indx[0]+x_prim_test.first_valid_index()\n",
    "misclassified_data['text'] = x_prim_test[misclassified_indx_corrected]\n",
    "misclassified_data['label'] = y_prim_test[misclassified_indx_corrected]\n",
    "misclassified_data['prediction'] = y_hat_prim_test[misclassified_indx]\n",
    "misclassified_data.reset_index(drop=True, inplace=True)\n",
    "misclassified_data.to_csv(\"/content/drive/MyDrive/Education/CS585/HW3/misclassified_primary_data.csv\",index=False)\n",
    "print(misclassified_data.head())\n",
    "\n",
    "y_test = np.asarray(y_sec_test)\n",
    "y_hat_sec_test = lr_model.predict(x_sec_test_transformed)\n",
    "misclassified_indx = np.where(y_test != y_hat_sec_test)\n",
    "misclassified_data = pd.DataFrame()\n",
    "misclassified_data['text'] = x_sec_test[misclassified_indx[0]]\n",
    "misclassified_data['label'] = y_sec_test[misclassified_indx[0]]\n",
    "misclassified_data['prediction'] = y_hat_sec_test[misclassified_indx[0]]\n",
    "misclassified_data.reset_index(drop=True, inplace=True)\n",
    "misclassified_data.to_csv(\"/content/drive/MyDrive/Education/CS585/HW3/misclassified_secondary_data.csv\",index=False)\n",
    "print(misclassified_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE-c6ON9j2ra"
   },
   "source": [
    "For the enhancement features we are going to use:\n",
    "1.   The presence of hashtag symbol\n",
    "2.   The presence of both words 'anti' and 'vaccine'\n",
    "3.   The presence of the sequence 'vax'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ccqrWaUK6A-Z"
   },
   "outputs": [],
   "source": [
    "pat = re.compile(r\"#\")\n",
    "def detect_hastag(text):\n",
    "  return int(len(pat.findall(text)) > 0)\n",
    "\n",
    "def contains_word(text, word):\n",
    "  return (' ' + word + ' ') in (' ' + text + ' ')\n",
    "\n",
    "def word1_and_word2(text,word1,word2):\n",
    "  return int((word1 in text) and (word2 in text))\n",
    "\n",
    "def contains_sequence(text, sequence):\n",
    "  return int((sequence) in (text))\n",
    "\n",
    "hash_tags_present_array_prim_train = x_prim_train.apply(detect_hastag)\n",
    "hash_tags_present_array_prim_test = x_prim_test.apply(detect_hastag)\n",
    "hash_tags_present_array_sec_test = x_sec_test.apply(detect_hastag)\n",
    "\n",
    "w1 = 'anti'\n",
    "w2 = 'vaccine'\n",
    "w1_and_w2_prim_train = x_prim_train.apply(word1_and_word2,args=(w1,w2))\n",
    "w1_and_w2_prim_test = x_prim_test.apply(word1_and_word2,args=(w1,w2))\n",
    "w1_and_w2_sec_test = x_sec_test.apply(word1_and_word2,args=(w1,w2))\n",
    "\n",
    "seq1 = 'vax'\n",
    "seq1_prim_train = x_prim_train.apply(contains_sequence,args=(seq1,))\n",
    "seq1_prim_test = x_prim_test.apply(contains_sequence,args=(seq1,))\n",
    "seq_sec_test = x_sec_test.apply(contains_sequence,args=(seq1,))\n",
    "\n",
    "# Add new features to the document representation:\n",
    "# train dataset\n",
    "x_prim_train_final = np.asarray(np.insert(x_prim_train_transformed.todense(), x_prim_train_transformed.shape[-1], \n",
    "  (hash_tags_present_array_prim_train,\n",
    "   w1_and_w2_prim_train,\n",
    "   seq1_prim_train\n",
    "   ), axis=1))\n",
    "# primary test dataset\n",
    "x_prim_test_final = np.asarray(np.insert(x_prim_test_transformed.todense(), x_prim_test_transformed.shape[-1], \n",
    "  (hash_tags_present_array_prim_test,\n",
    "   w1_and_w2_prim_test,\n",
    "   seq1_prim_test\n",
    "   ), axis=1))\n",
    "# secondary test dataset\n",
    "x_sec_test_final = np.asarray(np.insert(x_sec_test_transformed.todense(), x_sec_test_transformed.shape[-1], \n",
    "  (hash_tags_present_array_sec_test,\n",
    "   w1_and_w2_sec_test,\n",
    "   seq_sec_test\n",
    "   ), axis=1))\n",
    "# train new model\n",
    "lr_model_enhanced = LogisticRegression().fit(x_prim_train_final, y_prim_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6w0qdo26CHi"
   },
   "source": [
    "**5. Model evaluation 2:** Calculate overall model accuracy for your new model's predictions on the Test (PRIMARY) set, and on the Test (SECONDARY) set.  Enter these values in the answer boxes provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ql3K6zlP6K8q",
    "outputId": "8c97ae11-071f-4d52-d36a-923644683e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced model accuracy on primary dataset: 84.84%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.68      0.78        98\n",
      "           1       0.82      0.96      0.88       146\n",
      "\n",
      "    accuracy                           0.85       244\n",
      "   macro avg       0.87      0.82      0.83       244\n",
      "weighted avg       0.86      0.85      0.84       244\n",
      "\n",
      "Enhanced model accuracy on secondary dataset: 76.00%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.81      0.74       375\n",
      "           1       0.84      0.73      0.78       525\n",
      "\n",
      "    accuracy                           0.76       900\n",
      "   macro avg       0.76      0.77      0.76       900\n",
      "weighted avg       0.77      0.76      0.76       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# enhanced model performance on primary dataset\n",
    "enhanced_accuracy_prim = lr_model_enhanced.score(x_prim_test_final,y_prim_test)\n",
    "print(\"Enhanced model accuracy on primary dataset: {:.2%}\\n\".format(enhanced_accuracy_prim))\n",
    "print(classification_report(y_prim_test, lr_model_enhanced.predict(x_prim_test_final)))\n",
    "# enhanced model performance on secondary dataset\n",
    "enhanced_accuracy_sec = lr_model_enhanced.score(x_sec_test_final,y_sec_test)\n",
    "print(\"Enhanced model accuracy on secondary dataset: {:.2%}\\n\".format(enhanced_accuracy_sec))\n",
    "print(classification_report(y_sec_test, lr_model_enhanced.predict(x_sec_test_final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDl9TUej6PGi"
   },
   "source": [
    "**6. Reflection:** Answer the questions on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-y2IktPM4mR5"
   },
   "source": [
    "---\n",
    "QUESTION 1\n",
    "What is your baseline model's overall accuracy for the Test (PRIMARY) and Test (SECONDARY) data sets?\n",
    "> Answer:  \n",
    "> Baseline accuracy of the model on primary dataset: 80.74%  \n",
    "> Baseline accuracy the model on secondary dataset: 72.44%  \n",
    "---\n",
    "QUESTION 2\n",
    "What is your enhanced (with new engineered features) model's overall accuracy for the Test (PRIMARY) and Test (SECONDARY) data sets?\n",
    "> Answer  \n",
    "> Enhanced model accuracy on primary dataset: 84.84%  \n",
    "> Enhanced model accuracy on secondary dataset: 76.00%\n",
    "---\n",
    "QUESTION 3\n",
    "What new features did you add to your model?\n",
    "> Answer:  \n",
    "> 1.   The presence of hashtag symbol  \n",
    "> 2.   The presence of both words 'anti' and 'vaccine'  \n",
    "> 3.   The presence of the sequence 'vax'\n",
    "---\n",
    "QUESTION 4\n",
    "Did your new features improve model performance?  Why do you think they did (or did not)?\n",
    "> Answer:  \n",
    "> Yes, new features improved overal accuracy of the model by 4.1% on primary test dataset and by 3.56% on secondary dataset  \n",
    "---\n",
    "QUESTION 5\n",
    "Did your model have better accuracy on the Test (PRIMARY) or Test (SECONDARY) data set?  Why?\n",
    "> Answer:  \n",
    "> The model has better accuracy on Test (PRIMARY) dataset. This is likely due to the fact that the model was trained on the texts sampled from PRIMARY dataset therefore on texts similar in qualities and features to the test dataset. Even though the topic of SECONDARY dataset was the same, it came from conceptually different source which differs significantly from what the model was trained on. For example, PRIMARY dataset had very little hashtags in it, while SECONDARY dataset had a lot of hashtags. therefore, the model was not trained enough to proccess SECONDARY dataset with the higher accuracy.\n",
    "---\n",
    "QUESTION 6\n",
    "Please attach the zip file with your modeling code (as a Jupyter notebook) and any supporting files needed to run it.\n",
    "> Answer:  \n",
    "See submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8zewezG6cya"
   },
   "source": [
    "**7. Code submission:** Create a zip file including your code as a Jupyter notebook and any necessary supporting files.  Submit the file as requested within the assignment."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Oleksandr-Shashkov-CS-585-HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
